---
title: "Homework 5"
author: "Frederick J. Boehm"
date: "November 3, 2020"
classoption: landscape
output:
  pdf_document: default
  html_document:
    df_print: paged
---

## Question 1

### Q1, part i

We see that 

$$
\begin{aligned}
f^*(y) &= \sup_x \left( xy - |x| \right)\\
&= 1_{|y| > 1}\infty
\end{aligned}
$$

To see that this is the case consider, for $|y| \le 1$ and $x >0$:

$$
\begin{aligned}
xy - |x| &= xy - x
&\le 0\\
\end{aligned}
$$
Additionally, we see that the supremum must be 0, rather than something smaller than zero, since, when $y=1$, 

$$xy - x = x-x = 0$$



### Q1, part ii

$$
\begin{aligned}
f^*(y) &= \sup_{x} \left( xy - f(x)\right)\\
&= \sup_{x} \left( xy - |x|^p\right)\\
&\implies \frac{d}{dx}\left(xy - |x|^p\right) = y - p |x|^{p-1} = 0\\
&\implies \frac{y}{p} = |x|^{p-1}\\
&\implies \hat x = \left(\frac{y}{p}\right) ^ {\frac{1}{p-1}}\\
& \implies f^*(y) = y\left( \frac{y}{p}\right) ^{\frac{1}{p-1}} - \left(\frac{y}{p}\right) ^ {\frac{p}{p-1}}
\end{aligned}
$$

### Q1, part iii

$$
\begin{aligned}
f^*(y) &= \sup_x\left(xy - f(x) \right)\\
&= \sup_x \left(xy - x\log x \right)\\
&\implies \frac{d}{dx}\left(xy - \log x  \right) = y - \log x - 1 = 0\\
& \implies y = \log x + 1 \\
& \implies \hat x = e^{y-1}\\
& \implies f^*(y) = ye^{y-1} - (y-1) e^{y-1} = e^{y-1}
\end{aligned}
$$

## Question 2

### 2, part a




### 2, part b

### 2, part c










## Question 3

### Q3, part i

we first calculate the marginal density of the x's. 

$$
\begin{aligned}
p(x_1, \ldots, x_n) &= \int p(x_1, \ldots, x_n)p(\theta)d\theta \\
&= \int \theta ^ {a-1 + \sum_{i=1}^n x_i} e^{-n\theta - b\theta}\left(\frac{b^a}{\Gamma(b)}\right)\left(\frac{1}{\prod_{i=1}^n}x_i!\right)d\theta\\
&= \left(\frac{b^a}{\Gamma(b)}\right) \left(\frac{1}{\prod_{i=1}^n x_i!} \right)\int \theta ^{a - 1 + \sum_{i=1}^nx_i} e^{-\theta(n + b)}\left(\frac{(n + b)^{a + \sum x_i}}{(n + b )^{a + \sum x_i}} \right)\left( \frac{\Gamma(n + b)}{\Gamma(n + b)}\right)d\theta\\
&= \left(\frac{b^a}{\Gamma(b)}\right) \left(\frac{\Gamma(n + b)}{(n + b)^{a + \sum x_i}} \right)\left(\frac{1}{\prod x_i!} \right)
\end{aligned}
$$
Note that the last equality follows from the observation that the integrand is a multiple of a gamma density. 

We then calculate the posterior density.

$$
\begin{aligned}
p(\theta|x_1, \ldots, x_n) &= \frac{p(x_1, \ldots, x_n | \theta)p(\theta)}{p(x_1, \ldots, x_n)} \\
&= \left(\frac{\theta ^ {\sum x_i}e^{-n\theta}}{\prod x_i!}\right)\left( \frac{b^a\theta ^ {a-1}e^{-b\theta}}{\Gamma(b)}\right)\left(\frac{\Gamma(b)}{b^a}\right)\left( \frac{(n + b)^ {a + \sum x_i}}{\Gamma(n + b)}\right)\left(\prod x_i!\right) \\
&= \frac{\theta ^ {a - 1 + \sum x_i}e^{-\theta (n + b)}(n + b)^{a + \sum x_i}}{\Gamma(n + b)}
\end{aligned}
$$

### Q3, part ii

We let $T$ denote the sufficient statistic and $\theta$ the canonical parameter.

We then have

$$
p(x | \theta) = h(x) \exp\left( \theta T - A(\theta)\right)
$$ 

and 

$$
p(\theta | \eta, \zeta) = \exp\left( \eta \theta - \zeta A(\theta) - B(\eta, \zeta)\right)
$$

We compute the posterior distribution:

$$
\begin{aligned}
p(\theta | x, \eta, \zeta) &= \frac{p(\theta | \eta, \zeta)p(x | \theta)}{\int p(\theta | \eta, \zeta)p(x | \theta)}\\
&= \frac{h(x) \exp\left(\theta T + \eta T - \zeta A(\theta) - B(
\eta, \zeta) - A(\theta)\right)}{\int p(\theta | \eta, \zeta)p(x | \theta)}\\
&= \frac{\exp\left((\theta + \eta) T - (\zeta + 1) A(\theta) - B(
\eta, \zeta)\right)}{\int \exp\left((\theta + \eta) T - (\zeta + 1) A(\theta) - B(
\eta, \zeta)\right)} \\
&= \frac{\exp\left((\theta + \eta) T - (\zeta + 1) A(\theta) \right)}{\int \exp\left((\theta + \eta) T - (\zeta + 1) A(\theta)\right) }
\end{aligned}
$$
We then observe that the posterior has a form similar to that of the prior.



### Q3, part iii






## Question 4

### Q4, part i

To get the first equality, we interchange the order of integration and differentiation
in the definition of $A_{\theta}$ before applying the (differentiation) chain rule.

In symbols, we write:

$$
\begin{aligned}
(A_{\theta})_{kl} &= \frac{\partial}{\partial \theta_k} \int f_l p d\nu \\
&= \int \frac{\partial}{\partial \theta_k}(f_l p) d\nu \\ 
& = \int f_l \frac{\partial p}{\partial \theta_k}d\nu + \int \frac{\partial f_l}{\partial \theta_k}p d\nu\\
&= \int f_l \frac{\partial p}{\partial \theta_k}d\nu \\
&= \int f_l \frac{\partial p}{\partial \theta_k}p (\frac{1}{p})d\nu \\
&= \int f_l \frac{\partial \log p}{\partial \theta_k}p d\nu \\
&= \int f_l W_k p d\nu \\
&= \mathbb{E}\left( f_l W_k\right)
\end{aligned}
$$



### Q4, part ii





### Q4, part iii

First we establish the equality $$A_{\theta} = \mathbb{E}\left([f - \mathbb{E}f][T - \mathbb{E}T] \right)$$ where $T$ is the sufficient statistic and we're working in an exponential family with natural parameter $\eta$ and cgf $B$.

We start at Equation 2.166:

$$
\begin{aligned}
A_{\theta} &= \mathbb{E}\left( [f - \mathbb{E}f][W - \mathbb{E}W]\right)\\
&= \mathbb{E}\left([f - \mathbb{E}f][(T - \frac{\partial B}{\partial \eta}) - \mathbb{E}(T - \frac{\partial B}{\partial \eta})] \right)\\
& = \mathbb{E}\left([f - \mathbb{E}f][T - \frac{\partial B}{\partial \eta}] \right) \\
&= \mathbb{E}\left([f - \mathbb{E}f][T - \mathbb{E}T] \right)
\end{aligned}
$$

We now justify the above equalities. First, observe that $W = T - \frac{\partial B}{\partial \eta}$. This follows from definition of our exponential family. 

Second, observe that $\mathbb{E}T = \frac{\partial B}{\partial \eta}$. This means that $\mathbb{E}(T - \frac{\partial B}{\partial \eta})$ is zero in the above equalities. Finally, we apply this relationship to get the third line equaling the fourth line. 

We then argue that $f=T$ implies that $A_{\theta} = I_{\theta}$, the fisher information. This follows from the definition of Fisher information. 





### Q4, part iv

We need to use the law of large numbers here. First, we verify the hypotheses of the strong law of large numbers.

Then we see that the expectation of $\widehat{(A_{\theta})_{kl}}$ is, in fact, $(A_{\theta})_{kl}$.




### Q4, part v




