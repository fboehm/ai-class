---
title: "Homework 4"
author: "Frederick J. Boehm"
date: "10/15/2020"
classoption: landscape
output:
  pdf_document: default
  html_document:
    df_print: paged
---

## Question 1



## Question 2

We calculate (where we take expectations with respect to the joint distribution):

$$
\begin{aligned}
D(P_{12}|| P_1P_2) &= \mathbb{E}\left( - \log(2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}) + \log(2\pi\sigma_1\sigma_2)  -\frac{1}{2(1-\rho^2)}\left[\frac{(X_1 - \mu_1)^2}{\sigma_1 ^ 2} + \frac{(X_2 - \mu_2^2)}{\sigma_2 ^ 2} - \frac{2\rho(X_1 - \mu_1)(X_2 - \mu_2)}{\sigma_1\sigma_2}\right] + \frac{(X_1 - \mu_1)^2}{\sigma_1 ^ 2} + \frac{(X_2 - \mu_2)^2}{\sigma_2 ^ 2}\right)\\
&= - \frac{1}{2}\log(1-\rho^2) - \frac{1}{2(1-\rho^2)}\left[ 1 + 1 - \frac{2\rho}{\sigma_1\sigma_2}\rho\sigma_1\sigma_2\right] + \frac{1}{2} + \frac{1}{2}\\
&= - \frac{1}{2}\log(1-\rho^2) - \frac{1}{2(1-\rho^2)}\left[ 2-2\rho^2\right] + 1\\
&= - \frac{1}{2}\log(1-\rho^2)
\end{aligned}
$$



## Question 3

### 3a

Let $w_{ij} = P_{12}(X_1 = i, X_2 = j)$. Then $\sum_iw_{ij} = q_j = P_{2}(X_2 = j)$ and $\sum_j w_{ij} = p_i = P_1(X_1 = i)$.

Then, 

$$
\begin{aligned}
I(X_1, X_2) &= \sum_{i,j}w_{ij}\log \frac{w_{ij}}{p_iq_j} \\
&= \sum_{i,j}w_{ij}\log w_{ij} - \sum_{i,j} w_{ij}\log p_i - \sum_{i,j} w_{ij} \log q_j \\
&= \sum_{i,j}w_{ij}\log w_{ij} - \sum_{i} p_{i}\log p_i - \sum_{j} q_{j} \log q_j \\
&= - H((X_1, X_2)) + H(X_1) + H(X_2)
\end{aligned}
$$


### 3b

$$
\begin{aligned}
I(X_1, X_2) &= \sum_{i,j}w_{ij}\log \frac{w_{ij}}{p_iq_j} \\
&= \sum_{i,j}w_{ij}\log \frac{u_{i|j}q_j}{p_iq_j} \\
&= \sum_{i,j}w_{ij}\log \frac{u_{i|j}}{p_i} \\
&= \sum_{i,j}w_{ij}\log u_{i|j} - \sum_{i,j}w_{ij}\log p_i\\
&= \sum_{i,j}w_{ij}\log u_{i|j} - \sum_{i}p_{i}\log p_i \\
&= H(X) - H(X|Y)
\end{aligned}
$$


## Question 4




## Question 5

## Question 6

### 6i

$$
\begin{aligned}
l(\theta) = \log L(\theta) = \log f_{\theta}(x) &= x\log \theta + (1-x)\log (1-\theta) \\
&\implies \frac{d}{d\theta} l = \frac{x}{\theta} - \frac{1-x}{1-\theta} = \frac{x- \theta}{\theta(1-\theta)} \\
&\implies \mathbb{E}\left(\frac{dl}{d\theta}\right)^2 = \mathbb{E}\left(\frac{(x-\theta)^2}{\theta^2(1-\theta)^2} \right)\\
&= \frac{Var(X)}{\theta ^ 2(1-\theta)^2} \\
&\implies \mathcal{I}(\theta) = \frac{1}{(1-\theta) \theta}
\end{aligned}
$$

### 6ii

We calculate partial derivatives:

$$
\begin{aligned}
\frac{\partial l}{\partial \mu} &= \frac{X - \mu}{\sigma ^ 2} \\
\frac{\partial l}{\partial \sigma} &= \frac{-1}{\sigma} + \frac{(X - \mu)^2}{\sigma ^ 3}\\
\frac{\partial^2 l}{\partial \mu^2} &= \frac{-1}{\sigma^2}\\
\frac{\partial^2 l}{\partial \mu\partial \sigma} &= \frac{-2(X - \mu)}{\sigma^3}\\
\frac{\partial^2 l}{\partial \sigma^2} &= \frac{1}{\sigma ^ 2} - \frac{3(X - \mu)^2}{\sigma ^ 4}
\end{aligned}
$$

Taking (negative) expectations of the three second partial derivatives, we see that we have:

$$
\mathcal{I}(\mu, \sigma) = \begin{pmatrix}
\frac{1}{\sigma^2} & 0 \\
0 & \frac{2}{\sigma ^ 2} 
\end{pmatrix}
$$



## Question 7

### 7, part i





For the second conclusion, we want to use the weak law of large numbers. We need to verify that 
the assumptions of the theorem are met.
The weak law of large numbers states that 




### 7, part ii

We need to show that $V_n^2$ has bounded differences.

$$
\begin{aligned}
V_n^2 &= \frac{1}{n-1}\sum_{i = 1}^n(Y_i - \bar Y) ^ 2\\
|V_n^2 - V_{n'}^2| &= \frac{1}{n^2(n - 1)}|\sum_{i = 1}^n(nY_i - \sum_{i=1}^n Y_i) ^ 2 - \sum_{i = 1}^n(nY_i - \bar Y') ^ 2
\end{aligned}
$$


### 7, part iii



## Question 8

### 8, part i

$$
|G(X) - G(X')| = \lvert\sup_{f \in \mathcal{F}}|\bar Y - \mathbb{E}\bar Y| - \sup_{g \in \mathcal{F}}|\bar W - \mathbb{E}\bar W|\rvert \le |\sup_{f\in\mathcal{F}}|Y - \mathbb{E}\bar Y|| \le 2K
$$


### 8, part ii

We showed in part i the bounded differences hypothesis for Theorem 2.14. 
Then 


$$
\begin{aligned}
\mathbb{P}\left( |Y - \mathbb{E}Y | \ge t\right) \le 2\exp\left(- \frac{2t^2}{\sum_{i=1}^nc_i^2}\right)
\end{aligned}
$$


